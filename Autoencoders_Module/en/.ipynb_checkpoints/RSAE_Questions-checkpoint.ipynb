{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQOsPSWZyQZx"
   },
   "source": [
    "# Movie recommendation using Autoencoder \n",
    "## IVADO - Online course on Recommender Systems \n",
    "\n",
    "Colab notebook content originally created for IVADO's workshop on Recommender Systems, August 2019. \n",
    "\n",
    "\n",
    "<b> Authors: </b>\n",
    "\n",
    "David Berger \n",
    "\n",
    "Laurent Charlin \n",
    "\n",
    "Nissan Pow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYuwYEztyQZ0"
   },
   "source": [
    "# 1. Introduction\n",
    "\n",
    "In this workshop, we will see how to implement recommendation systems based on an autoencoder (AE), a classic architecture in deep learning. As with the *Matrix factorization* tutorial, we will use the <a href=\"https://grouplens.org/datasets/movielens/\">MovieLens</a> database to train the models and conduct experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDKq8-yXyQZ1"
   },
   "source": [
    "## 1.1 Installing libraries\n",
    "\n",
    "Before we begin, we must make sure to install the libraries for the tutorial using `pip`. To do this, run the following cell by selecting it and then press the `shift`+`Enter` keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "and193IDyQZ2",
    "outputId": "d28084dc-4660-4d24-fb72-902d66eaa737"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'RS-Workshop'...\n",
      "remote: Enumerating objects: 49, done.\u001b[K\n",
      "remote: Counting objects: 100% (49/49), done.\u001b[K\n",
      "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
      "remote: Total 49 (delta 14), reused 47 (delta 12), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (49/49), done.\n"
     ]
    }
   ],
   "source": [
    "!rm -rf RS-Workshop\n",
    "!git clone https://github.com/davidberger2785/RS-Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXnUNiZPyQZ8"
   },
   "source": [
    "To ensure that the installation has been done properly, import all the libraries and modules that we will use for this workshop by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffIsFyBmyQZ8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data vizualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Homemade functions\n",
    "sys.path += ['RS-Workshop/Tutoriels - En/']\n",
    "import utilities as utl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjO04i-MyQZ_"
   },
   "source": [
    "We have also coded some boilerplate functions that we have grouped together in the `utilities` library. In fact, these functions probably already exist in Python, but we often simply ignore they exist..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bt2C99HCyQZ_"
   },
   "source": [
    "## 1.2 Goal\n",
    "\n",
    "The purpose of recommender systems is, as the name implies, to make personalized recommendations. Ideally, these recommendations will have to be good, although this concept may seem fuzzy. Unlike other tasks in machine learning, for example cat images recognition or stock market predictions, making recommendations to help a user is more complex. Is the aim to present a specific user with suggestions that reinforce their previous choices? Do we want to present complementary or totally independent suggestions for the items previously considered? Will we try instead to present them items to which they have not yet been exposed? Each of these options are correct and can be modeled. Without loss of generality, the diagram below simply models the issue of recommendation systems from the perspective of machine learning.\n",
    "\n",
    "![title](https://github.com/davidberger2785/RS-Workshop/blob/master/Images/High_level_1.png?raw=1)\n",
    "\n",
    "\n",
    "\n",
    "Nevertheless, in the context of this workshop, by using the example of movies available on streaming platforms such as Netflix or Amazon Prime, we can reduce the problem to a relatively simple task: recommend movies that the user will like according to their past interests. In order to carry out this task, we will use all the preferences of the users, certain associated sociodemographic variables as well as certain characteristics of the movies. Finally, we can refine the diagram as follows:\n",
    "\n",
    "![title](https://github.com/davidberger2785/RS-Workshop/blob/master/Images/High_level_2.png?raw=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8kkOcZLyQaA"
   },
   "source": [
    "## 1.3 The MoviesLens dataset(s)\n",
    "\n",
    "The data used here consists of approximately 100k movie ratings by 943 users. Over 1,6k movies are available. Beyond the 100k ratings, additional information related to users and movies is available.\n",
    "\n",
    "We will use three different datasets to carry out our analyses:\n",
    "\n",
    "*   Users : related to users' characteristics,\n",
    "*   Movies : related to movies' characteristics,\n",
    "*   Ratings : containing over 100k ratings.\n",
    "\n",
    "We are using the <a href=\"https://pandas.pydata.org/\">Pandas</a> library in order to download and manipulate the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apGD8Z_CyQaA"
   },
   "source": [
    "### 1.3.1 Users: Download and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F6pLOxHjyQaB"
   },
   "outputs": [],
   "source": [
    "# Download\n",
    "ROOT_DIR = 'RS-Workshop/'\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data/ml-100k/')\n",
    "\n",
    "users = pd.read_csv(os.path.join(DATA_DIR, 'u.user'), \n",
    "                        sep='|', header=None, engine='python', encoding='latin-1')\n",
    "\n",
    "# Different variables according to the information provided in the readme file\n",
    "users.columns = ['Index', 'Age', 'Gender', 'Occupation', 'Zip code']\n",
    "\n",
    "# Quick overview\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xg1jIhbyQaD"
   },
   "source": [
    "Before presenting some descriptive statistics related to the population, we will format the users' data as a <a href=\"https://en.wikipedia.org/wiki/List_(abstract_data_type)\">list</a> to handle it more easily. The occupation of each user is a string, so we have recoded the 21 possible occupations in boolean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C7KCPwC9yQaE"
   },
   "outputs": [],
   "source": [
    "# Number of users\n",
    "nb_users = len(users)\n",
    "\n",
    "# Gender\n",
    "gender = np.where(np.matrix(users['Gender']) == 'M', 0, 1)[0]\n",
    "\n",
    "# Occupation\n",
    "occupation_name = np.array(pd.read_csv(os.path.join(DATA_DIR, 'u.occupation'), \n",
    "                                            sep='|', header=None, engine='python', encoding='latin-1').loc[:, 0])\n",
    "\n",
    "# Boolean transformation of user's occupation\n",
    "occupation_matrix = np.zeros((nb_users, len(occupation_name)))\n",
    "\n",
    "for k in np.arange(nb_users):\n",
    "    occupation_matrix[k, occupation_name.tolist().index(users['Occupation'][k])] = 1\n",
    "\n",
    "# Concatenation of the sociodemographic variables \n",
    "user_attributes = np.concatenate((np.matrix(users['Age']), np.matrix(gender), occupation_matrix.T)).T.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MupJaXiZyQaG"
   },
   "source": [
    "We then explore the descriptive statistics of the users. These include information related to *age* (continuous variable), *gender* (binary variable) and *occupation* of each user (21, all binary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMm2LDjSyQaG"
   },
   "source": [
    "Descriptive statistics related to <i>Age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZF4Y5TUQyQaH"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(users['Age'].describe()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeX_TqtCyQaJ"
   },
   "source": [
    "Barplot graph related to <i>Gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ktjctlNPyQaJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "utl.barplot(['Women', 'Men'], np.array([np.mean(gender) , 1 - np.mean(gender)]) * 100, \n",
    "            'Sex', 'Percentage (%)', \"User's gender\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tv5sHwVZyQaO"
   },
   "source": [
    "Barplot graph related to <i>Occupation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z0_b6pd9yQaP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attributes, scores = utl.rearrange(occupation_name, np.mean(occupation_matrix, axis=0) * 100)\n",
    "utl.barplot(attributes, scores, 'Occupation', 'Percentage (%)', \"User's occupation\", 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyIfNODfyQaR"
   },
   "source": [
    "### 1.3.2 Movies: Download and preprocesssing\n",
    "\n",
    "We will now process and explore the data associated with movies. For each movie, we have the *title*, the *release date* (in North America), as well as the corresponding *genres*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Em0nC2FpyQaR"
   },
   "outputs": [],
   "source": [
    "movies = pd.read_csv(os.path.join(DATA_DIR, 'u.item'), sep='|', header=None, engine='python', encoding='latin-1')\n",
    "\n",
    "# Number of movies\n",
    "nb_movies = len(movies)\n",
    "\n",
    "# Genres\n",
    "movies_genre = np.matrix(movies.loc[:, 5:])\n",
    "movies_genre_name = np.array(pd.read_csv(os.path.join(DATA_DIR, 'u.genre'), sep='|', header=None, engine='python', encoding='latin-1').loc[:, 0])\n",
    "\n",
    "# Quick overview\n",
    "movies.columns = ['Index', 'Title', 'Release', 'The Not a Number column', 'Imdb'] + movies_genre_name.tolist()\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0uDnLOYyQaT"
   },
   "source": [
    "We present the ratio of movies according to the genre as a descriptive statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eqaRcdBtyQaU"
   },
   "outputs": [],
   "source": [
    "attributes, scores = utl.rearrange(movies_genre_name, \n",
    "                                   np.array(np.round(np.mean(movies_genre, axis=0) * 1, 2))[0])\n",
    "utl.barplot(attributes, np.array(scores) * 100, xlabel='Genre', ylabel='Percentage (%)', \n",
    "            title=\" \", rotation = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BIFt6pxyQaW"
   },
   "source": [
    "### 1.3.3 Ratings: Download and preprocessing\n",
    "\n",
    "The dataset based on users ratings consists of approximately 100k lines (one evaluation per line) where the following are presented: \n",
    "\n",
    "*   the *user identification number*\n",
    "*   the *identification number of the movie*\n",
    "*   its associated *rating*\n",
    "*   *a time marker*\n",
    "\n",
    "The training and test sets were provided as is (we do not need to build them ourselves) and have 80k and 20k evaluations respectively.\n",
    "\n",
    "For practical reasons, we convert the database as a list using our `convert` util function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SlMXfiltyQaX"
   },
   "outputs": [],
   "source": [
    "training_set = np.array(pd.read_csv(os.path.join(DATA_DIR, 'u1.base'), delimiter='\\t'), dtype='int')\n",
    "testing_set = np.array(pd.read_csv(os.path.join(DATA_DIR, 'u1.test'), delimiter='\\t'), dtype='int')\n",
    "\n",
    "train_set = utl.convert(training_set, nb_users, nb_movies)\n",
    "test_set = utl.convert(testing_set, nb_users, nb_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zvO5UCTyQaZ"
   },
   "source": [
    "As we did before, we can get descriptive statistics associated with the evaluations. At first, it might be interesting to study the average trends of users.\n",
    "\n",
    "##### Question 1\n",
    "\n",
    "1. What other types of statistics might be interesting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qu-AEw28yQaZ"
   },
   "outputs": [],
   "source": [
    "train_matrix = np.array(train_set)\n",
    "shape = (len(train_set), len(train_set[0]))\n",
    "train_matrix.reshape(shape)\n",
    "train_matrix_bool = np.where(train_matrix > 0 , 1, 0)\n",
    "\n",
    "user_watch = np.sum(train_matrix_bool, axis=1)\n",
    "pd.DataFrame(user_watch).describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9Dsm_LZyQac"
   },
   "source": [
    "Histogram of the number of movies watched per user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQhKtRRFyQad"
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(12,8)})\n",
    "sns.set(font_scale = 1.5)\n",
    "\n",
    "plt.title('Empirical distribution of \\n the number of movies watched per user')\n",
    "plt.xlabel('Number of movies watched')\n",
    "plt.ylabel('Number of users')\n",
    "plt.hist(user_watch, 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJNTBhdeyQag"
   },
   "source": [
    "Descriptive statistics related to the movies' evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZXYJ4eryQah"
   },
   "outputs": [],
   "source": [
    "movie_frequency = np.mean(train_matrix_bool, axis=0)\n",
    "pd.DataFrame(movie_frequency).describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67pC2NGKyQaj"
   },
   "source": [
    "##### Question 2\n",
    "\n",
    "a. What statistics or observations might we consider relevant? Why?\n",
    "\n",
    "b. What kind of statistics might be more appropriate in such a context?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4KxdnsTyQaj"
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Proportion of the population who watched the movie')\n",
    "plt.ylabel('Number of Movies')\n",
    "plt.hist(movie_frequency, 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qfpcfvk_yQal"
   },
   "source": [
    "#### Individual preferences according to the type of movie\n",
    "\n",
    "We could also look at the behavior of a particular individual. Among other things, we could study if there is a bias associated with their evaluation scheme or what are their cinematographic preferences according to the score awarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74RuNFKtyQal"
   },
   "outputs": [],
   "source": [
    "def stats_user(data, movies_genre, user_id):\n",
    "    \n",
    "    ratings = data[user_id]\n",
    "    stats = np.zeros(6)\n",
    "    eva = np.zeros((6, movies_genre.shape[1]))\n",
    "\n",
    "    for k in np.arange(len(ratings)):\n",
    "        index = int(ratings[k])\n",
    "        stats[index] += 1\n",
    "        eva[index, :] = eva[index, :] + movies_genre[k]\n",
    "\n",
    "    return stats, eva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9sH_M2HyQan"
   },
   "outputs": [],
   "source": [
    "user_id = 0\n",
    "stats, eva = stats_user(train_set, movies_genre, user_id)\n",
    "utl.barplot(np.arange(5) + 1, stats[1:6] / sum(stats[1:6]), xlabel='Number of stars', ylabel='Percentage of movies (%)', \n",
    "            title=\" \", rotation = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zuIGfhUFyQaq"
   },
   "source": [
    "##### Question 3\n",
    "\n",
    "3. How can we test to the existence of bias associated to individual's assessment scheme?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4jcmvzwyQaq"
   },
   "source": [
    "## 1.4 Construction of the training and validation sets\n",
    "\n",
    "In machine learning, we manipulate <a href=\"https://blogs.nvidia.com/blog/2018/04/15/nvidia-research-image-translation/\">complex databases</a> for which we attempt to define equally complex function spaces in order to accomplish a specific task. That being said, these function spaces are defined by a set of parameters whose number tends to increase with the complexity of the data. Once space is defined by a set of fixed parameters, we can vary the different values of the hyperparameters in order to empirically explore function spaces. To choose the set of optimal parameters and hyperparameters, we define a metric that allows us to evaluate the model; for example, how much the image of a cat seems likely.\n",
    "\n",
    "To the extent that we want to develop a model that can generalize, the evaluation of its performance must be done on an independent dataset, coming from the same distribution from the set on which the model was trained. This set is called the validation set.\n",
    "\n",
    "**! Note !** \n",
    "\n",
    "The notion of training and test set in the framework of recommender systems is somewhat different from what is usually seen with so-called supervised problems. If in the context of a supervised problem, the test set consists essentially of new observations (lines from a file) which are independent of observations previously observed in the training set. The paradigm is significantly different when we work with recommendation systems.\n",
    "\n",
    "Indeed, because of the mathematical model on which the recommendation systems are based, the data belonging to the test set are not linked to a new individual, but rather to new evaluations made by the same set of individuals. As a result, the data associated with the training, validation and test sets are no longer independent as assumed (the famous *iid* hypothesis) which complicates things theoretically.\n",
    "\n",
    "Since the purpose of the workshop is not to study the notion of bias associated with the type of dependence between the different assessments in referral systems, we will naively assume that each of the assessments is independent of each other. Nevertheless, in a practical framework, ignoring this kind of considerations will possibly bias the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vvqL3zJmyQaq"
   },
   "outputs": [],
   "source": [
    "def split(data, ratio, tensor=False):\n",
    "    train = np.zeros((len(data), len(data[0]))).tolist()\n",
    "    valid = np.zeros((len(data), len(data[0]))).tolist()\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])):\n",
    "            if data[i][j] > 0:\n",
    "                if np.random.binomial(1, ratio, 1):\n",
    "                    train[i][j] = data[i][j]\n",
    "                else:\n",
    "                    valid[i][j] = data[i][j]\n",
    "\n",
    "    return [train, valid]\n",
    "\n",
    "train_0 = split(train_set, 0.8)\n",
    "test = test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tYJWzOVyQaw"
   },
   "source": [
    "# 2. Autoencoder as recommender systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPQ8-o41yQax"
   },
   "source": [
    "## 2.1 Model\n",
    "\n",
    "In general, auto-encoders are a class of neural networks that allow unsupervised learning of the latent characteristics of the data being studied. To do this, the AE will attempt to predict, or copy, the input observations using (multiple) hidden layer. In its simplest form, the architecture of an AE can be summarized in the diagram below.\n",
    "\n",
    "![title](https://github.com/davidberger2785/RS-Workshop/blob/master/Images/AE.png?raw=1)\n",
    "\n",
    "Looking more closely, the AE consists of an encoder, the function $h(\\cdot)$ defined by:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    h(\\mathbf{x}) = \\frac{1}{1+ \\exp(-\\mathbf{W} \\mathbf{x})}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This function takes as input the observations and will consist of recoding it as a hidden layer so as to reduce their size (fewer neurons). Afterwards, an encoder defined by:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    f(h(\\mathbf{x})) = \\mathbf{W}^\\top h(\\mathbf{x})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "will attempt *to reconstruct* the input observations from the hidden layer. In this sense, the AE tries to estimate the observations used as input.\n",
    "\n",
    "We can advantageously use the estimates made by the auto-encoders in such a way as to present new recommendations to the users.\n",
    "\n",
    "For example, suppose that the set of evaluations made by a user is defined by the vector:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathbf{x} = [3, \\ 0, \\ 0, \\ 1, \\ ..., \\ 2, \\ 4].\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We are noticing that the movie occupying the first position, *Toy Story*, was moderately appreciated, while the next two movies, *Golden Eye* and *Four Room*, have not been viewed at all. Suppose once again that in this same set of assessments, the EA will present the following estimates:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathbf{\\hat{x}} = [3.2, \\ 1.3, \\ 4, \\ 0.5, \\ ..., \\ 3, \\ 1].\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "As a result, we will be able to use the estimates associated with initially unvisited movies as recommendations. Thus, the movie *Four Rooms*, in third position, seems a good suggestion for the user, while *Golden Eye* is definitely not a convincing recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tb8oH65vyQax"
   },
   "source": [
    "## 2.2 Deep learning with Pytorch\n",
    "\n",
    "In order to build a recommendation system based on autoencoders, we will use the\n",
    "<a href=\"https://pytorch.org/\">Pytorch</a> library. It provides two extremely interesting features:\n",
    "<ul>\n",
    "<li> Manipulation of tensors (kind of multidimensional matrices) to perform calculations with GPU. </li>\n",
    "<li> Automatic differentiation (!!!) with the <a href=\"http://pytorch.org/docs/master/autograd.html\">autograd class</a> to easily calculate the gradient descent. </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dJgvL_1yQay"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "from torch.nn import functional\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8KzAwHKyQaz"
   },
   "source": [
    "Since we will work with Pytorch, let's turn the MoviesLens dataset into a tensor object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EjAhxXU6yQa0"
   },
   "outputs": [],
   "source": [
    "train = torch.FloatTensor(train_0)\n",
    "train, valid = train[0], train[1]\n",
    "test = torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTz99dilyQa2"
   },
   "source": [
    "**! Note !** \n",
    "\n",
    "Although the documentation available for Pytorch is detailed (compared to other deep learning libraries), it is easy to get lost. Nevertheless, for this workshop, it is not necessary to enter all the details associated with the different commands. In fact, the key is to understand the ins and outs of the key steps presented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgHOkq9yyQa2"
   },
   "source": [
    "## 2.3 Implementation\n",
    "\n",
    "We can decline in 5 steps the implementation of an AE as a recommendation system:\n",
    "\n",
    "1. Initialization of the AE,\n",
    "2. Propagation of the message,\n",
    "3. Estimation: cost calculation and retropropagation,\n",
    "4. Learning loop,\n",
    "5. Evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8afngqjXyQa2"
   },
   "source": [
    "### 2.3.1 Initialization\n",
    "\n",
    "First, let's define the autoencoder class using the <a href=\"http://pytorch.org/docs/master/nn.html#module\"> torch.nn</a> module. In PyTorch, any neural network must inherit this class. The autoencoder uses other common classes in Pytorch, such as <a href = \"http://pytorch.org/docs/master/nn.html#torch.nn. Linear \"> torch.nn.Linear (in_features, out_features)</a>. The latter implements a fully connected linear layer (as its name suggests).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pw5N1JF0Ca1L"
   },
   "source": [
    "##### Question 4\n",
    "\n",
    "4. Complete the initialization of the autoencoder class according to the architecture diagram presented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-RVCLuGayQa3"
   },
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self, inputs, outputs, features, criterion=None):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "           self: class name\n",
    "           nb_inputs: number of neurons on the input layer\n",
    "           nb_outputs: number of neurons on the output layer\n",
    "           nb_features: number of neurons on the hidden layer\n",
    "           criterion: loss function used for learning  \n",
    "        \"\"\"\n",
    "        \n",
    "        super(AE, self).__init__()\n",
    "        # Complete this line \n",
    "        # And this one too\n",
    "        \n",
    "        self.criterion = criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ozLRChh6yQa5"
   },
   "source": [
    "We then define:\n",
    "\n",
    "1. The number of neurons input,\n",
    "2. The number of neurons output,\n",
    "3. The number of neurons desired in the hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXv9KoeQyQa5"
   },
   "source": [
    "##### Question 5\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRMLG7iqDMfX"
   },
   "source": [
    "###### Question 5.1\n",
    "Initialize the autoencoder with the correct parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJXLOMYZyQa6"
   },
   "outputs": [],
   "source": [
    "nb_inputs = ?\n",
    "nb_outputs = ?\n",
    "nb_features = ?\n",
    "\n",
    "ae = AE(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJWe85RbDDlX"
   },
   "source": [
    "###### Question 5.2\n",
    "\n",
    "Is it relevant that the hidden layer has more neurons than the input layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TyH3ax3HyQa9"
   },
   "source": [
    "### 2.3.2 Propagation\n",
    "\n",
    "During the propagation phase, the `forward` function associated with the propagation of the message defines the operations to be performed in order to calculate the elements of the output. This function is essential and must match the initialization of the model in the previous step to allow proper backpropagation. \n",
    "   \n",
    "Note the use of the <a href=\"http://pytorch.org/docs/master/nn.html#torch-nn-functional\">torch.nn.functional</a> method which define a set of functions which can be applied to the layers of a neural network. In this workshop, we will use non-linear functions like <a href=\"http://pytorch.org/docs/master/nn.html#id36\">sigmoid</a> and cost functions such as the mean squared error <a href=\"http://pytorch.org/docs/master/nn.html#mse-loss\">mse_loss</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOpI_qz9yQa-"
   },
   "source": [
    "##### Question 6\n",
    "\n",
    "6. Write down the `forward` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w4qq6sRCyQa-"
   },
   "outputs": [],
   "source": [
    "def forward(model, x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model: name of the autoencoder as initialized\n",
    "        x: input layer, here made up of 1682 neurons\n",
    "    Return:\n",
    "        predictions: output layer\n",
    "    \"\"\"\n",
    "\n",
    "    # TO-DO\n",
    "    \n",
    "    return ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXgbMhgVyQbB"
   },
   "source": [
    "### 2.2.2 Weight estimates\n",
    "\n",
    "Although neural networks have breathtaking predictive capabilities, the complexity of their architecture can become high very quickly. From a computational point of view, among other things, this translates into the impossibility of obtaining an optimum regards to the loss function. Of course, estimating the weights analytically is mostly impossible, as can be done in a linear model under normality assumption for example. Nevertheless, if no global optimum is guaranteed, and if incidentally no analytical form can be calculated, the associated weights can be estimated.\n",
    "\n",
    "That being said, the (stochastic) gradient descent (and its derivatives) is an efficient optimization technique that is mainly used in deep learning. This technique uses three key concepts:\n",
    "\n",
    "1. Cost function,\n",
    "2. Optimizer's type,\n",
    "3. Gradient backpropagation (implemented in the learning loop)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZyXo169lyQbB"
   },
   "source": [
    "#### 2.2.2.1 Loss function\n",
    "\n",
    "As we saw in the previous workshop, the loss function plays an important role in the construction of a predictive model. In fact, it is this same loss function that we try to minimize (or maximize) by iteratively adjusting the weights of the AE. Thus, two different loss functions will most likely result in two different models. As usual, Pytorch offers a large amount of <a href=\"http://pytorch.org/docs/master/nn.html#id42\">loss functions</a> that you can explore at your leisure.\n",
    "\n",
    "Since ratings vary between 1 and 5, the mean square error (MSE) seems an interesting first option. Formally, as part of a recommendation system, we will define the MSE as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textit{MSE}(\\mathbf{R}, \\hat{\\mathbf{R}}) = \\frac{1}{n} \\sum_{r_{ui} \\neq 0} (r_{ui} - \\hat{r}_{ui})^2, \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{R}$ and $\\hat{\\mathbf{R}} $ are respectively the matrices of the observed and predicted ratings and $n$ is the number of estimates. In the same way, $r_{ui}$ and $\\hat{r}_{ui} $ are scalars associated respectively with the observed evaluation and the estimate of the user $u$ for the item $i$.\n",
    "\n",
    "Since we have encoded the loss function as an attribute of the autoencoder class, we define it with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EEXLzacKyQbC"
   },
   "outputs": [],
   "source": [
    "ae.criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGMAyx5TyQbD"
   },
   "source": [
    "##### Question 7\n",
    "\n",
    "7. The MSE is an interesting loss function for recommendation system with explicit data. Which relevant loss function could have been implemented if the data had been preference-based binary ratings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plmRF7m8yQbE"
   },
   "source": [
    "#### 2.2.2.2 Optimizer\n",
    "\n",
    "PyTorch provides several <a href=\"http://pytorch.org/docs/master/optim.html#algorithms\">optimization methods</a> more or less derived from the gradient descent via the `torch 'class. optim`. Among these techniques:\n",
    "\n",
    "<ul>\n",
    "<li> SGD (Stochastic Gradient Descent): implementation of SGD.\n",
    "<li> Adam (Adaptive Moment Estimation): variation of the gradient descent method where the learning rate is adjusted for each parameter.\n",
    "<li> RMSprop: actual optimizer for this workshop. For more details <a href=\"http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf\"> here</a>.\n",
    "</ul>\n",
    "\n",
    "In any case, when we use iterative optimization methods, we must provide a learning rate and a weight decay value, for reasons similar to those mentioned in the <i>Matrix factorization</i> workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AUF_1V9FyQbE"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.02\n",
    "weight_decay = 0.2\n",
    "\n",
    "optimizer = optim.RMSprop(ae.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFiTJAkkyQbH"
   },
   "source": [
    "#### 2.2.2.3 Backpropagation\n",
    "\n",
    "In Pytorch, the gradient's backpropagation is simplified thanks to the automatic differentiation and the <a href=\"http://pytorch.org/docs/master/notes/autograd.html\">autograd</a> class. This is done in two steps:\n",
    "\n",
    "1. Calculation of the loss function with the function previously defined in the class of the AE.\n",
    "2. Automatic differentiation of the loss function with the `backward()` function.\n",
    "\n",
    "The entire backpropagation process is directly implemented in the learning loop defined below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IZE4qcSyQbH"
   },
   "source": [
    "### 2.2.3 Learning loop\n",
    "\n",
    "When an autoencoder (and generally a deep learning-based architecture) is used as a recommendation system, the learning loop differs somewhat from the matrix factorization-based models. Thus, each rating is no longer considered individually, as it was the case before, but is considered on the whole of the ratings provided by a specific user.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5l865Yw4JC4g"
   },
   "source": [
    "##### Question 8\n",
    "\n",
    "a. Complete the propagation phase.\n",
    "\n",
    "b. At the end of each epoch, which statistic would it be better to calculate? Code it. Note: initialize objects at the beginning of the function (see line 4)\n",
    "\n",
    "c. Implement the backpropagation phase.\n",
    "\n",
    "d. Since data from the training, validation or test set can be used in the fit function, what conditions should we put on line 22?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mdAUWt8vyQbH"
   },
   "outputs": [],
   "source": [
    "def fit(model, x, y, valid=False):\n",
    "    \n",
    "    nb_obs, nb_items = len(x), len(x[0])\n",
    "    average_loss, s = 0, 0.\n",
    "\n",
    "    for id_user in range(nb_obs):\n",
    "\n",
    "        inputs = Variable(x[id_user]).unsqueeze(0)\n",
    "        target = Variable(y[id_user]).unsqueeze(0)\n",
    "\n",
    "        if torch.sum(target > 0) > 0:\n",
    "            \n",
    "            # Question 8.1: Estimate\n",
    "            estimate = ?\n",
    "            # TO-DO\n",
    "            target.require_grad = False\n",
    "            \n",
    "            # Question 8.3: Retropropagation\n",
    "            loss = ?\n",
    "            \n",
    "            # Question 8.4: Condition\n",
    "            if ?:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Question 8.2: Statistics\n",
    "            # TO-DO\n",
    "            s += 1.\n",
    "\n",
    "    return model, average_loss, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYyw2GVlyQbJ"
   },
   "source": [
    "## 2.4 Training\n",
    "\n",
    "The autoencoder and the associated functions now implemented, we can start to train the model. Once again, the goal here is not to tune the parameters so as to obtain the best possible model, but simply to understand the role that they can play according to the model's predictive ability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvN6ja4XJJx1"
   },
   "source": [
    "##### Question 9\n",
    "\n",
    "9. Finish implementing the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-ydW5_KyQbJ"
   },
   "outputs": [],
   "source": [
    "nb_epoch = 20\n",
    "\n",
    "for epoch in range(1, nb_epoch + 1):\n",
    "    \n",
    "    # TO-DO\n",
    " \n",
    "    print(\"epoch: \", \"{:3.0f}\".format(epoch), \"   |   train: \", \"{:1.8f}\".format(train_loss.numpy() / train_s), \\\n",
    "                    '   |   valid: ', \"{:1.8f}\".format(valid_loss.numpy() / valid_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrsVTVVeyQbM"
   },
   "source": [
    "You can now manipulate the various parameters and hyperparameters of the AE. Among the various modifications that you can make, here is a (short and not exhaustive) list of easily implementable modifications:\n",
    "\n",
    "1. Change the hyperparameters (a bit boring).\n",
    "2. Increase the size of the hidden layer (more interesting).\n",
    "3. Add hidden layers to the model, making sure to initialize them and adapt the forward function.\n",
    "4. Dichotomize the ratings using a threshold (3 for example) and run the whole code by adapting or not the cost function as discussed previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tcquuEUyQbM"
   },
   "source": [
    "Finally, we can evaluate the performance of our model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHCODYiGyQbN"
   },
   "outputs": [],
   "source": [
    "ae, test_loss, test_s = fit(model=ae, x=test, y=test, valid=True)\n",
    "print('test: ',\"{:1.8f}\".format(test_loss.numpy() / test_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-ZvlGKkyQbQ"
   },
   "source": [
    "## 2.5 Analysis\n",
    "\n",
    "\n",
    "### 2.5.1 Exploration of the latent layer\n",
    "\n",
    "In a similar way to what was presented in the matrix factorization workshop, we can explore the latent layer of AE. Insofar as the input layer represents the set of evaluations for a given individual, each neuron in the latent layer will be associated with a latent attribute of an individual.\n",
    "\n",
    "As an example, let's say $\\mathbf{H}_{|U| \\times k}$ the matrix associated with the tent layer where each row represents the latent representation of the preferences for a given user and where each column represents a neuron of that same layer. Suppose now that the first two neurons of the latent layer have the following values:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h_1 &= [1.0, \\ 0.0, \\ -0.5, \\ ..., \\ 1.0, \\ -1.0]\n",
    "\\qquad \\text{and} \\qquad\n",
    "h_2 = [1.0, \\ 0.0, \\ 0.5, \\ ..., \\ -1.0, \\ -0.8].\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "And that to these values correspond the following users:\n",
    "\n",
    "1. Serena,\n",
    "2. Kali,\n",
    "3. Neil,\n",
    "4. Marie,\n",
    "5. David.\n",
    "\n",
    "We can then map users based on the values ​​associated with $h_1$ and $h_2$:\n",
    "\n",
    "![title](https://github.com/davidberger2785/RS-Workshop/blob/master/Images/hidden_4.png?raw=1)\n",
    "\n",
    "\n",
    "This approach could allow us to perform clusters of individuals based on latent attributes. In fact, a quick glance allows us to think that the preferences in terms of cinema of Serena are the opposite of David, which is not very surprising.\n",
    "\n",
    "With the following commands, we propose to further explore the latent structure of AE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rzlHsEykyQbQ"
   },
   "outputs": [],
   "source": [
    "x = train\n",
    "hidden = torch.sigmoid(ae.fc1(x)).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_oQUsAByQbT"
   },
   "source": [
    "We could be interested, for example, in measures of association between the different hidden layers, or a hidden layer and a socio-demographic characteristics. To do this, an interesting avenue would be to simply calculate the associated correlations.\n",
    "\n",
    "First, by simply exploring the correlations between different neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebCk3BucyQbT"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.array(hidden))\n",
    "f = plt.figure(figsize=(6, 6))\n",
    "plt.matshow(df.corr(), fignum=f.number)\n",
    "plt.xticks(range(df.shape[1]), df.columns, fontsize=10, rotation=0)\n",
    "plt.yticks(range(df.shape[1]), df.columns, fontsize=10)\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-r0v_i7yQbV"
   },
   "source": [
    "##### Question 10\n",
    "\n",
    "10. Which neuron of the hidden layer seems <i>a priori </i> the most interesting? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsY2vDgayQbV"
   },
   "source": [
    "##### Question 11\n",
    "\n",
    "11. Latent layer study was done here according to the users of the system. Would it be possible to study the latent layers associated with individuals. If so, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NNUvFz2yQbW"
   },
   "source": [
    "# 3. Applications\n",
    "\n",
    "One of the primary objectives of the recommendation systems is to make personalized recommendations. Therefore, it might be interesting to study the recommendations made by our model for a specific individual. It would also be preferable that the recommendations made suggest only films not viewed by the user.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-5m512lJoXl"
   },
   "source": [
    "##### Question 12\n",
    "\n",
    "12. Implement a short function which presents the best *k* recommendations that have not yet been viewed by a specific user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IPXEtoAdyQbW"
   },
   "outputs": [],
   "source": [
    "def recommendations(model, data, titles, k):\n",
    "    \"\"\"\n",
    "     Args:\n",
    "         model: name of the autoencoder as initialized\n",
    "         data: assessments associated with the user\n",
    "         titles: list of titles of potentially recommended objects\n",
    "         k: number of recommendations wanted\n",
    "     Return:\n",
    "         names: names of the recommendations\n",
    "         scores: the score associated with them\n",
    "     \"\"\"\n",
    "    # TO-DO\n",
    "    \n",
    "    return ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gkPtOvMIyQbY"
   },
   "source": [
    "Call of the function with a few manipulations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l1PVUkKyyQbY"
   },
   "outputs": [],
   "source": [
    "user_id = 0\n",
    "k=10\n",
    "names, scores = recommendations(ae, (train + valid + test)[user_id], movies['Title'], k)\n",
    "\n",
    "df = pd.DataFrame(np.matrix((names[-k:], scores[-k:])).T, (np.arange(k) + 1).tolist())\n",
    "df.columns = ['Title', 'Predicted rating']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8JoYDYdyQba"
   },
   "source": [
    "As we saw in the workshop on matrix-based recommendation systems, we can customize algorithms with different parameters. For example, we could implement a system with the best recommendations based on:\n",
    "\n",
    "1. Of a particular genre of movie.\n",
    "2. A minimum preferred quality: a minimum predicted score strictly greater than 4.5, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohnEsX7myQba"
   },
   "source": [
    "##### Question 13\n",
    "\n",
    "13. Do different algorithms make identical recommendations? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9T5Zy189yQbb"
   },
   "source": [
    "# 4. Other modeling ideas\n",
    "\n",
    "So far, we have only considered the evaluations in our model. It might be interesting to consider other types of modeling.\n",
    "\n",
    "For example, instead of using film ratings made by an individual as an input layer (hence 1682 neurons), we could use the ratings of individuals for a particular movie (and thus 943 input layer neurons). In this modeling, we could incorporate the different types of films and/or their year of release.\n",
    "\n",
    "Finally, we could simply get ourselves away from autoencoders and ogle other types of architectures. Considering the different functions and class previously coded, we could implement a multilayer perceptron (MLP). To do this, the inputs of the network would be exactly the same, with the difference that the targets would consist only of unvisited films."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kWSAmBEyQbc"
   },
   "source": [
    "## 4.1 Use of socio-demographic informations\n",
    "\n",
    "It might be interesting to check whether the use of socio-demographic informations improves or not the predictive capabilities of the model. To the extent that such information would only improve the capabilities of the model very little, they could be useful when a new user intends to use the recommendation system already in place. Although imperfect, the information associated with the age, gender and occupation of a user could be useful for presenting the first recommendations.\n",
    "\n",
    "In order to observe how the recommendation system behaves with such data, we must first modify the different sets so that they present the socio-demographic information of each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mzq15h9TyQbc"
   },
   "outputs": [],
   "source": [
    "train_inputs = torch.FloatTensor(utl.inner_concatenation(user_attributes, train_0[0]))\n",
    "train_outputs = torch.FloatTensor(train_0[0])\n",
    "\n",
    "valid_inputs = torch.FloatTensor(utl.inner_concatenation(user_attributes, train_0[1]))\n",
    "valid_outputs = torch.FloatTensor(train_0[1])\n",
    "\n",
    "test_inputs = torch.FloatTensor(utl.inner_concatenation(user_attributes, test_set))\n",
    "test_outputs = torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-KippqOyQbh"
   },
   "source": [
    "##### Question 14\n",
    "\n",
    "14. Initialize the autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F07Bdo-xyQbh"
   },
   "outputs": [],
   "source": [
    "#TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6MteKllyQbj"
   },
   "source": [
    "##### Question 15\n",
    "\n",
    "15. Code the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uGREkaBCyQbk"
   },
   "outputs": [],
   "source": [
    "#TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYhkBa1ryQbm"
   },
   "source": [
    "##### Question 16\n",
    "\n",
    "16. Calculate the performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G6XQvsfwyQbm"
   },
   "outputs": [],
   "source": [
    "#TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0IwUVsx4yQbo"
   },
   "source": [
    "### 4.1.1 Cold start problem\n",
    "\n",
    "As previously mentioned, beyond improving the performance of the model according to the chosen metric, the use of socio-demographic variables in the model makes it possible to make recommendations to a new user simply according to its attributes. This modeling can counteract the cold start problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ef4d7Y_oJ1zb"
   },
   "source": [
    "##### Question 17\n",
    "\n",
    "1. Set the age, gender and occupation of an individual.\n",
    "\n",
    "2. Consider that the latter has not yet evaluated any movie.\n",
    "\n",
    "3. Show her/him, according to the estimated model, the best movie recommendations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43OfivOOyQbo"
   },
   "outputs": [],
   "source": [
    "# Question 17.1: \n",
    "age = [?]\n",
    "gender = [?]\n",
    "\n",
    "occupation = np.zeros(len(occupation_name))\n",
    "\n",
    "occupation[occupation_name.tolist().index('artist')] = 1\n",
    "\n",
    "# Question 17.2: \n",
    "  #TO-DO\n",
    "\n",
    "# Question 17.3: \n",
    "  #TO-DO"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "gFiTJAkkyQbH"
   ],
   "name": "RSAE - Questions.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
